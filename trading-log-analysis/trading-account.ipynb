{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536c6b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import psycopg2\n",
    "from psycopg2.extras import RealDictCursor\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import gmail_api\n",
    "import base64\n",
    "import email\n",
    "from email.mime.text import MIMEText\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from email.mime.base import MIMEBase\n",
    "from email import encoders\n",
    "import io\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25dbd792",
   "metadata": {},
   "outputs": [],
   "source": [
    "EQUITY_BASE = 'S3121200 - Equity Base'\n",
    "WPAC_STATEMENT = 'Wpac Statement data'\n",
    "\n",
    "trading_account = pd.read_excel('Wentworth Stock & Trading Futures Account.xls', sheet_name=EQUITY_BASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951b5365",
   "metadata": {},
   "outputs": [],
   "source": [
    "trading_account.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b60b5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "trading_account.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f81bb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database Configuration\n",
    "DB_CONFIG = {\n",
    "    'host': 'localhost',\n",
    "    'port': 5432,\n",
    "    'user': 'zeerakwyne',\n",
    "    'password': '',\n",
    "    'database': 'wentworth'\n",
    "}\n",
    "\n",
    "# Gmail Configuration (you'll need to set these up)\n",
    "GMAIL_CREDENTIALS = {\n",
    "    'credentials_file': 'credentials.json',  # Download from Google Cloud Console\n",
    "    'token_file': 'token.json'  # Will be created after first auth\n",
    "}\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4840da8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_database():\n",
    "    \"\"\"Create the wentworth database if it doesn't exist\"\"\"\n",
    "    try:\n",
    "        # Connect to default postgres database to create wentworth\n",
    "        conn = psycopg2.connect(\n",
    "            host=DB_CONFIG['host'],\n",
    "            port=DB_CONFIG['port'],\n",
    "            user=DB_CONFIG['user'],\n",
    "            password=DB_CONFIG['password'],\n",
    "            database='postgres'  # Connect to default postgres db\n",
    "        )\n",
    "        conn.autocommit = True\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Check if database exists\n",
    "        cursor.execute(\"SELECT 1 FROM pg_database WHERE datname = 'wentworth'\")\n",
    "        exists = cursor.fetchone()\n",
    "        \n",
    "        if not exists:\n",
    "            cursor.execute(\"CREATE DATABASE wentworth\")\n",
    "            logger.info(\"Database 'wentworth' created successfully\")\n",
    "        else:\n",
    "            logger.info(\"Database 'wentworth' already exists\")\n",
    "            \n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error creating database: {e}\")\n",
    "        return False\n",
    "\n",
    "# Test database creation\n",
    "create_database()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9946a49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_excel_from_database_updated(output_file=None):\n",
    "    \"\"\"Generate Excel file from database data - Updated to match original format\"\"\"\n",
    "    try:\n",
    "        conn = psycopg2.connect(**DB_CONFIG)\n",
    "        \n",
    "        # Query all data from database\n",
    "        query = \"\"\"\n",
    "        SELECT \n",
    "            business_date, starting_cash, daily_charges, profit_and_loss, \n",
    "            daily_transfers, daily_received_cash, daily_cash_paid, cash, \n",
    "            open_trade_equity, total_equity, net_liquidation_value, \n",
    "            initial_margin, maintenance_margin, excess_deficit, \n",
    "            mtd_profit_and_loss, ytd_profit_and_loss\n",
    "        FROM equity_data \n",
    "        ORDER BY business_date DESC;\n",
    "        \"\"\"\n",
    "        \n",
    "        df = pd.read_sql_query(query, conn)\n",
    "        conn.close()\n",
    "        \n",
    "        # Rename columns to match original Excel format exactly\n",
    "        column_mapping = {\n",
    "            'business_date': 'Business Date (Start)',\n",
    "            'starting_cash': 'Starting Cash',\n",
    "            'daily_charges': 'Daily Charges',\n",
    "            'profit_and_loss': 'Profit and Loss',\n",
    "            'daily_transfers': 'Daily Transfers',\n",
    "            'daily_received_cash': 'Daily Received Cash',\n",
    "            'daily_cash_paid': 'Daily Cash Paid',\n",
    "            'cash': 'Cash',\n",
    "            'open_trade_equity': 'Open Trade Equity',\n",
    "            'total_equity': 'Total Equity',\n",
    "            'net_liquidation_value': 'Net Liquidation Value',\n",
    "            'initial_margin': 'Initial Margin',\n",
    "            'maintenance_margin': 'Maintenance Margin',\n",
    "            'excess_deficit': 'Excess Deficit',\n",
    "            'mtd_profit_and_loss': 'MTD Profit and Loss',\n",
    "            'ytd_profit_and_loss': 'YTD Profit and Loss'\n",
    "        }\n",
    "        \n",
    "        df = df.rename(columns=column_mapping)\n",
    "        \n",
    "        # Add the additional columns that are in the original Excel but not in our database\n",
    "        # These will be empty/NaN for now, but we can populate them later if needed\n",
    "        additional_columns = {\n",
    "            'Gross Trading P&L': df['Profit and Loss'],  # Same as Profit and Loss\n",
    "            'Net Trading P&L': df['Profit and Loss'],    # Same as Profit and Loss  \n",
    "            'Open Trade Equity ': df['Open Trade Equity'],  # Note the trailing space\n",
    "            'Daily Futures P&L': df['Profit and Loss']   # Same as Profit and Loss\n",
    "        }\n",
    "        \n",
    "        # Add the additional columns\n",
    "        for col_name, col_data in additional_columns.items():\n",
    "            df[col_name] = col_data\n",
    "        \n",
    "        # Reorder columns to match original Excel format\n",
    "        column_order = [\n",
    "            'Business Date (Start)', 'Starting Cash', 'Daily Charges', 'Profit and Loss',\n",
    "            'Daily Transfers', 'Daily Received Cash', 'Daily Cash Paid', 'Cash',\n",
    "            'Open Trade Equity', 'Total Equity', 'Net Liquidation Value', 'Initial Margin',\n",
    "            'Maintenance Margin', 'Excess Deficit', 'MTD Profit and Loss', 'YTD Profit and Loss',\n",
    "            'Gross Trading P&L', 'Net Trading P&L', 'Open Trade Equity ', 'Daily Futures P&L'\n",
    "        ]\n",
    "        \n",
    "        df = df[column_order]\n",
    "        \n",
    "        # Generate output filename if not provided\n",
    "        if output_file is None:\n",
    "            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "            output_file = f'../data/Wentworth_Stock_Trading_Futures_Account_{timestamp}.xlsx'\n",
    "        \n",
    "        # Create Excel file with multiple sheets\n",
    "        with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
    "            # Main equity data sheet\n",
    "            df.to_excel(writer, sheet_name=EQUITY_BASE, index=False)\n",
    "            \n",
    "            # Summary sheet\n",
    "            summary_data = {\n",
    "                'Total Records': [len(df)],\n",
    "                'Date Range': [f\"{df['Business Date (Start)'].min()} to {df['Business Date (Start)'].max()}\"],\n",
    "                'Last Updated': [datetime.now().strftime('%Y-%m-%d %H:%M:%S')],\n",
    "                'Account ID': ['S3121200'],\n",
    "                'Legal Account Name': ['WENTWORTH STOCK AND TRADING PTY LTD']\n",
    "            }\n",
    "            summary_df = pd.DataFrame(summary_data)\n",
    "            summary_df.to_excel(writer, sheet_name='Summary', index=False)\n",
    "        \n",
    "        logger.info(f\"Excel file generated successfully: {output_file}\")\n",
    "        return output_file\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error generating Excel file: {e}\")\n",
    "        return None\n",
    "\n",
    "# Test the updated Excel generation\n",
    "# generate_excel_from_database_updated()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37296d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_equity_table():\n",
    "    \"\"\"Create the equity_data table with proper schema\"\"\"\n",
    "    try:\n",
    "        conn = psycopg2.connect(**DB_CONFIG)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Create table with all columns from CSV\n",
    "        create_table_query = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS equity_data (\n",
    "            id SERIAL PRIMARY KEY,\n",
    "            business_date DATE NOT NULL,\n",
    "            managing_location VARCHAR(50),\n",
    "            account_id VARCHAR(50),\n",
    "            family_group_code_1 VARCHAR(50),\n",
    "            legal_account_name VARCHAR(255),\n",
    "            currency VARCHAR(10),\n",
    "            starting_cash DECIMAL(15,2),\n",
    "            daily_charges DECIMAL(15,2),\n",
    "            daily_tax DECIMAL(15,2),\n",
    "            daily_option_premiums DECIMAL(15,2),\n",
    "            profit_and_loss DECIMAL(15,2),\n",
    "            daily_transfers DECIMAL(15,2),\n",
    "            daily_received_cash DECIMAL(15,2),\n",
    "            daily_cash_paid DECIMAL(15,2),\n",
    "            cash DECIMAL(15,2),\n",
    "            open_trade_equity DECIMAL(15,2),\n",
    "            total_equity DECIMAL(15,2),\n",
    "            net_option_value DECIMAL(15,2),\n",
    "            net_liquidation_value DECIMAL(15,2),\n",
    "            initial_margin DECIMAL(15,2),\n",
    "            maintenance_margin DECIMAL(15,2),\n",
    "            excess_deficit DECIMAL(15,2),\n",
    "            mtd_profit_and_loss DECIMAL(15,2),\n",
    "            ytd_profit_and_loss DECIMAL(15,2),\n",
    "            forward_cash_entries DECIMAL(15,2),\n",
    "            forward_futures_pl DECIMAL(15,2),\n",
    "            forward_charges DECIMAL(15,2),\n",
    "            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "            UNIQUE(business_date, account_id)\n",
    "        );\n",
    "        \"\"\"\n",
    "        \n",
    "        cursor.execute(create_table_query)\n",
    "        \n",
    "        # Create index on business_date for efficient querying\n",
    "        cursor.execute(\"CREATE INDEX IF NOT EXISTS idx_equity_business_date ON equity_data(business_date);\")\n",
    "        cursor.execute(\"CREATE INDEX IF NOT EXISTS idx_equity_account_id ON equity_data(account_id);\")\n",
    "        \n",
    "        conn.commit()\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        \n",
    "        logger.info(\"Equity data table created successfully\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error creating equity table: {e}\")\n",
    "        return False\n",
    "\n",
    "# Create the table\n",
    "create_equity_table()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0c56f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Let's examine the actual Excel file structure\n",
    "# excel_file = '../data/Wentworth Stock & Trading Futures Account.xls'\n",
    "# df_excel = pd.read_excel(excel_file, sheet_name=EQUITY_BASE)\n",
    "\n",
    "# print(\"Excel file shape:\", df_excel.shape)\n",
    "# print(\"\\nColumn names:\")\n",
    "# print(df_excel.columns.tolist())\n",
    "# print(\"\\nFirst few rows:\")\n",
    "# print(df_excel.head())\n",
    "# print(\"\\nData types:\")\n",
    "# print(df_excel.dtypes)\n",
    "\n",
    "\n",
    "def migrate_excel_to_database_fixed():\n",
    "    \"\"\"Migrate existing Excel data to PostgreSQL database - FIXED VERSION\"\"\"\n",
    "    try:\n",
    "        # Read the existing Excel file\n",
    "        excel_file = 'Wentworth Stock & Trading Futures Account.xls'\n",
    "        df = pd.read_excel(excel_file, sheet_name=EQUITY_BASE)\n",
    "        \n",
    "        logger.info(f\"Loaded {len(df)} rows from Excel file\")\n",
    "        logger.info(f\"Columns: {df.columns.tolist()}\")\n",
    "        \n",
    "        # Clean the data - remove rows with null business dates\n",
    "        df_clean = df.dropna(subset=['Business Date (Start)'])\n",
    "        logger.info(f\"After cleaning: {len(df_clean)} rows\")\n",
    "        \n",
    "        # Connect to database\n",
    "        conn = psycopg2.connect(**DB_CONFIG)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Prepare data for insertion - mapping Excel columns to database columns\n",
    "        insert_query = \"\"\"\n",
    "        INSERT INTO equity_data (\n",
    "            business_date, managing_location, account_id, family_group_code_1, \n",
    "            legal_account_name, currency, starting_cash, daily_charges, daily_tax,\n",
    "            daily_option_premiums, profit_and_loss, daily_transfers, daily_received_cash,\n",
    "            daily_cash_paid, cash, open_trade_equity, total_equity, net_option_value,\n",
    "            net_liquidation_value, initial_margin, maintenance_margin, excess_deficit,\n",
    "            mtd_profit_and_loss, ytd_profit_and_loss, forward_cash_entries,\n",
    "            forward_futures_pl, forward_charges\n",
    "        ) VALUES (\n",
    "            %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s\n",
    "        ) ON CONFLICT (business_date, account_id) DO NOTHING;\n",
    "        \"\"\"\n",
    "        \n",
    "        # Convert DataFrame to list of tuples with proper mapping\n",
    "        data_tuples = []\n",
    "        for _, row in df_clean.iterrows():\n",
    "            # Map Excel columns to database columns\n",
    "            data_tuple = (\n",
    "                row.get('Business Date (Start)'),  # business_date\n",
    "                'SINSFT',  # managing_location (default value)\n",
    "                'S3121200',  # account_id (default value)\n",
    "                None,  # family_group_code_1\n",
    "                'WENTWORTH STOCK AND TRADING PTY LTD',  # legal_account_name (default value)\n",
    "                'AUD',  # currency (default value)\n",
    "                row.get('Starting Cash'),  # starting_cash\n",
    "                row.get('Daily Charges'),  # daily_charges\n",
    "                None,  # daily_tax\n",
    "                None,  # daily_option_premiums\n",
    "                row.get('Profit and Loss'),  # profit_and_loss\n",
    "                row.get('Daily Transfers'),  # daily_transfers\n",
    "                row.get('Daily Received Cash'),  # daily_received_cash\n",
    "                row.get('Daily Cash Paid'),  # daily_cash_paid\n",
    "                row.get('Cash'),  # cash\n",
    "                row.get('Open Trade Equity'),  # open_trade_equity\n",
    "                row.get('Total Equity'),  # total_equity\n",
    "                None,  # net_option_value\n",
    "                row.get('Net Liquidation Value'),  # net_liquidation_value\n",
    "                row.get('Initial Margin'),  # initial_margin\n",
    "                row.get('Maintenance Margin'),  # maintenance_margin\n",
    "                row.get('Excess Deficit'),  # excess_deficit\n",
    "                row.get('MTD Profit and Loss'),  # mtd_profit_and_loss\n",
    "                row.get('YTD Profit and Loss'),  # ytd_profit_and_loss\n",
    "                None,  # forward_cash_entries\n",
    "                None,  # forward_futures_pl\n",
    "                None   # forward_charges\n",
    "            )\n",
    "            data_tuples.append(data_tuple)\n",
    "        \n",
    "        # Insert data\n",
    "        cursor.executemany(insert_query, data_tuples)\n",
    "        conn.commit()\n",
    "        \n",
    "        # Get count of inserted records\n",
    "        cursor.execute(\"SELECT COUNT(*) FROM equity_data\")\n",
    "        count = cursor.fetchone()[0]\n",
    "        \n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        \n",
    "        logger.info(f\"Successfully migrated {count} records to database\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error migrating Excel data: {e}\")\n",
    "        return False\n",
    "\n",
    "# Migrate the data with the fixed function\n",
    "migrate_excel_to_database_fixed()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3864cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_gmail_api():\n",
    "    \"\"\"Initialize Gmail API connection\"\"\"\n",
    "    try:\n",
    "        from gmail_api import GmailAPI\n",
    "        gmail = GmailAPI(\n",
    "            credentials_file='../credentials.json',\n",
    "            token_file='../token.json'\n",
    "        )\n",
    "        logger.info(\"Gmail API setup successful\")\n",
    "        return gmail\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error setting up Gmail API: {e}\")\n",
    "        return None\n",
    "\n",
    "# Initialize Gmail API (you'll need to set up credentials first)\n",
    "gmail = setup_gmail_api()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09431de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_csv_data(csv_content):\n",
    "    \"\"\"Process CSV content and return DataFrame\"\"\"\n",
    "    try:\n",
    "        from io import StringIO\n",
    "        df = pd.read_csv(StringIO(csv_content))\n",
    "        \n",
    "        # Clean column names (remove spaces, convert to lowercase)\n",
    "        df.columns = df.columns.str.replace(' ', '_').str.lower()\n",
    "        \n",
    "        # Convert Business Date to datetime\n",
    "        df['business_date'] = pd.to_datetime(df['business_date'])\n",
    "        \n",
    "        logger.info(f\"Processed CSV with {len(df)} rows\")\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing CSV data: {e}\")\n",
    "        return None\n",
    "\n",
    "def insert_csv_to_database(df):\n",
    "    \"\"\"Insert CSV data into database\"\"\"\n",
    "    try:\n",
    "        conn = psycopg2.connect(**DB_CONFIG)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Prepare data for insertion\n",
    "        insert_query = \"\"\"\n",
    "        INSERT INTO equity_data (\n",
    "            business_date, managing_location, account_id, family_group_code_1, \n",
    "            legal_account_name, currency, starting_cash, daily_charges, daily_tax,\n",
    "            daily_option_premiums, profit_and_loss, daily_transfers, daily_received_cash,\n",
    "            daily_cash_paid, cash, open_trade_equity, total_equity, net_option_value,\n",
    "            net_liquidation_value, initial_margin, maintenance_margin, excess_deficit,\n",
    "            mtd_profit_and_loss, ytd_profit_and_loss, forward_cash_entries,\n",
    "            forward_futures_pl, forward_charges\n",
    "        ) VALUES (\n",
    "            %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s\n",
    "        ) ON CONFLICT (business_date, account_id) DO UPDATE SET\n",
    "            managing_location = EXCLUDED.managing_location,\n",
    "            family_group_code_1 = EXCLUDED.family_group_code_1,\n",
    "            legal_account_name = EXCLUDED.legal_account_name,\n",
    "            currency = EXCLUDED.currency,\n",
    "            starting_cash = EXCLUDED.starting_cash,\n",
    "            daily_charges = EXCLUDED.daily_charges,\n",
    "            daily_tax = EXCLUDED.daily_tax,\n",
    "            daily_option_premiums = EXCLUDED.daily_option_premiums,\n",
    "            profit_and_loss = EXCLUDED.profit_and_loss,\n",
    "            daily_transfers = EXCLUDED.daily_transfers,\n",
    "            daily_received_cash = EXCLUDED.daily_received_cash,\n",
    "            daily_cash_paid = EXCLUDED.daily_cash_paid,\n",
    "            cash = EXCLUDED.cash,\n",
    "            open_trade_equity = EXCLUDED.open_trade_equity,\n",
    "            total_equity = EXCLUDED.total_equity,\n",
    "            net_option_value = EXCLUDED.net_option_value,\n",
    "            net_liquidation_value = EXCLUDED.net_liquidation_value,\n",
    "            initial_margin = EXCLUDED.initial_margin,\n",
    "            maintenance_margin = EXCLUDED.maintenance_margin,\n",
    "            excess_deficit = EXCLUDED.excess_deficit,\n",
    "            mtd_profit_and_loss = EXCLUDED.mtd_profit_and_loss,\n",
    "            ytd_profit_and_loss = EXCLUDED.ytd_profit_and_loss,\n",
    "            forward_cash_entries = EXCLUDED.forward_cash_entries,\n",
    "            forward_futures_pl = EXCLUDED.forward_futures_pl,\n",
    "            forward_charges = EXCLUDED.forward_charges,\n",
    "            created_at = CURRENT_TIMESTAMP;\n",
    "        \"\"\"\n",
    "        \n",
    "        # Convert DataFrame to list of tuples\n",
    "        data_tuples = []\n",
    "        for _, row in df.iterrows():\n",
    "            data_tuple = (\n",
    "                row.get('business_date'),\n",
    "                row.get('managing_location'),\n",
    "                row.get('account_id'),\n",
    "                row.get('family_group_code_1'),\n",
    "                row.get('legal_account_name'),\n",
    "                row.get('currency'),\n",
    "                row.get('starting_cash'),\n",
    "                row.get('daily_charges'),\n",
    "                row.get('daily_tax'),\n",
    "                row.get('daily_option_premiums'),\n",
    "                row.get('profit_and_loss'),\n",
    "                row.get('daily_transfers'),\n",
    "                row.get('daily_received_cash'),\n",
    "                row.get('daily_cash_paid'),\n",
    "                row.get('cash'),\n",
    "                row.get('open_trade_equity'),\n",
    "                row.get('total_equity'),\n",
    "                row.get('net_option_value'),\n",
    "                row.get('net_liquidation_value'),\n",
    "                row.get('initial_margin'),\n",
    "                row.get('maintenance_margin'),\n",
    "                row.get('excess_deficit'),\n",
    "                row.get('mtd_profit_and_loss'),\n",
    "                row.get('ytd_profit_and_loss'),\n",
    "                row.get('forward_cash_entries'),\n",
    "                row.get('forward_futures_pl'),\n",
    "                row.get('forward_charges')\n",
    "            )\n",
    "            data_tuples.append(data_tuple)\n",
    "        \n",
    "        # Insert data\n",
    "        cursor.executemany(insert_query, data_tuples)\n",
    "        conn.commit()\n",
    "        \n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        \n",
    "        logger.info(f\"Successfully inserted {len(data_tuples)} records to database\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error inserting CSV data: {e}\")\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b470ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_emails_by_date_range(start_date, end_date):\n",
    "    \"\"\"Download emails for a date range and process them\"\"\"\n",
    "    try:\n",
    "        gmail = setup_gmail_api()\n",
    "        if not gmail:\n",
    "            logger.error(\"Failed to setup Gmail API\")\n",
    "            return False\n",
    "        \n",
    "        # Convert string dates to datetime objects\n",
    "        if isinstance(start_date, str):\n",
    "            start_date = datetime.strptime(start_date, '%Y-%m-%d')\n",
    "        if isinstance(end_date, str):\n",
    "            end_date = datetime.strptime(end_date, '%Y-%m-%d')\n",
    "        \n",
    "        # Get emails for date range\n",
    "        emails = gmail.get_emails_by_date_range(start_date, end_date)\n",
    "        \n",
    "        if not emails:\n",
    "            logger.warning(f\"No emails found for date range {start_date} to {end_date}\")\n",
    "            return False\n",
    "        \n",
    "        success_count = 0\n",
    "        for date_str, message in emails:\n",
    "            try:\n",
    "                # Download CSV data\n",
    "                csv_data = gmail.download_csv_attachment(message['id'])\n",
    "                if csv_data:\n",
    "                    # Process CSV\n",
    "                    df = process_csv_data(csv_data.decode('utf-8'))\n",
    "                    if df is not None:\n",
    "                        # Insert to database\n",
    "                        if insert_csv_to_database(df):\n",
    "                            success_count += 1\n",
    "                            logger.info(f\"Successfully processed email for {date_str}\")\n",
    "                        else:\n",
    "                            logger.error(f\"Failed to insert data for {date_str}\")\n",
    "                    else:\n",
    "                        logger.error(f\"Failed to process CSV for {date_str}\")\n",
    "                else:\n",
    "                    logger.error(f\"No CSV data found for {date_str}\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing email for {date_str}: {e}\")\n",
    "        \n",
    "        logger.info(f\"Successfully processed {success_count} out of {len(emails)} emails\")\n",
    "        return success_count > 0\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error downloading emails: {e}\")\n",
    "        return False\n",
    "\n",
    "# Example usage:\n",
    "download_emails_by_date_range('2025-09-26', '2025-10-10')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67243f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def daily_automation():\n",
    "    \"\"\"Daily automation script to download today's email and update database\"\"\"\n",
    "    try:\n",
    "        logger.info(\"Starting daily automation process...\")\n",
    "        \n",
    "        # Get today's date\n",
    "        today = datetime.now().strftime('%Y%m%d')\n",
    "        today_date = datetime.now().strftime('%Y-%m-%d')\n",
    "        \n",
    "        logger.info(f\"Processing data for {today_date}\")\n",
    "        \n",
    "        # Setup Gmail API\n",
    "        gmail = setup_gmail_api()\n",
    "        if not gmail:\n",
    "            logger.error(\"Failed to setup Gmail API\")\n",
    "            return False\n",
    "        \n",
    "        # Download today's email\n",
    "        csv_data = gmail.get_daily_equity_data(today)\n",
    "        if not csv_data:\n",
    "            logger.warning(f\"No email found for today ({today_date})\")\n",
    "            return False\n",
    "        \n",
    "        # Process CSV data\n",
    "        df = process_csv_data(csv_data)\n",
    "        if df is None:\n",
    "            logger.error(\"Failed to process CSV data\")\n",
    "            return False\n",
    "        \n",
    "        # Insert to database\n",
    "        if not insert_csv_to_database(df):\n",
    "            logger.error(\"Failed to insert data to database\")\n",
    "            return False\n",
    "        \n",
    "        # Generate updated Excel file\n",
    "        excel_file = generate_excel_from_database()\n",
    "        if not excel_file:\n",
    "            logger.error(\"Failed to generate Excel file\")\n",
    "            return False\n",
    "        \n",
    "        logger.info(f\"Daily automation completed successfully. Excel file: {excel_file}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in daily automation: {e}\")\n",
    "        return False\n",
    "\n",
    "# Run daily automation\n",
    "daily_automation()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8979ee7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_database_stats():\n",
    "    \"\"\"Get statistics about the database\"\"\"\n",
    "    try:\n",
    "        conn = psycopg2.connect(**DB_CONFIG)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Get total count\n",
    "        cursor.execute(\"SELECT COUNT(*) FROM equity_data\")\n",
    "        total_count = cursor.fetchone()[0]\n",
    "        \n",
    "        # Get date range\n",
    "        cursor.execute(\"SELECT MIN(business_date), MAX(business_date) FROM equity_data\")\n",
    "        date_range = cursor.fetchone()\n",
    "        \n",
    "        # Get latest record\n",
    "        cursor.execute(\"SELECT business_date, account_id, total_equity FROM equity_data ORDER BY business_date DESC LIMIT 1\")\n",
    "        latest_record = cursor.fetchone()\n",
    "        \n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        \n",
    "        stats = {\n",
    "            'total_records': total_count,\n",
    "            'date_range': date_range,\n",
    "            'latest_record': latest_record\n",
    "        }\n",
    "        \n",
    "        logger.info(f\"Database Stats: {stats}\")\n",
    "        return stats\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error getting database stats: {e}\")\n",
    "        return None\n",
    "\n",
    "# Get database statistics\n",
    "get_database_stats()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb78061",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_database_stats()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "forecasting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
